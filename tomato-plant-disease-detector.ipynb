{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":544347,"sourceType":"datasetVersion","datasetId":259770}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project : Skin Disease Recognition\n\n## DataSet Description: \n\nThe images are meticulously categorized into 22 distinct classes, each corresponding to a specific skin condition:\n\n- Acne\n- Actinic Keratosis\n- Benign Tumors\n- Bullous\n- Candidiasis\n- Drug Eruption\n- Eczema\n- Infestations/Bites\n- Lichen\n- Lupus\n- Moles\n- Psoriasis\n- Rosacea\n- Seborrheic Keratoses\n- Skin Cancer\n- Sun/Sunlight Damage\n- Tinea\n- Unknown/Normal\n- Vascular Tumors\n- Vasculitis\n- Vitiligo\n- Warts\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# Import necessary libraries\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Flatten\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.preprocessing.image import img_to_array,array_to_img,ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:45:46.750642Z","iopub.execute_input":"2025-01-04T12:45:46.750924Z","iopub.status.idle":"2025-01-04T12:45:46.755681Z","shell.execute_reply.started":"2025-01-04T12:45:46.750902Z","shell.execute_reply":"2025-01-04T12:45:46.754628Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Accessing the dataset","metadata":{}},{"cell_type":"code","source":"# train and test paths\ntrain_dir = \"/kaggle/input/tomato/New Plant Diseases Dataset(Augmented)/train\"\ntest_dir = \"/kaggle/input/tomato/New Plant Diseases Dataset(Augmented)/valid\"\n\n# creating ImageDataGenerator for train and test\ntrain_generator = ImageDataGenerator(\n    rescale = 1.0/255,\n    rotation_range = 20,\n    width_shift_range =0.2,\n    height_shift_range = 0.2,\n    zoom_range = 0.2,\n    shear_range = 0.2,\n    horizontal_flip = True\n    \n)\ntest_generator = ImageDataGenerator(\n    rescale = 1.0/255\n)\n\n# Accessing the train and test data by performing data Augmentation\ntrain_data = train_generator.flow_from_directory(\n    train_dir,\n    target_size = (224,224),\n    batch_size = 32,\n    class_mode = \"categorical\",\n    seed = 123\n)\n\ntest_data = test_generator.flow_from_directory(\n    test_dir,\n    target_size = (224,224),\n    batch_size = 32,\n    class_mode = \"categorical\",\n    seed = 123\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:45:50.633752Z","iopub.execute_input":"2025-01-04T12:45:50.634045Z","iopub.status.idle":"2025-01-04T12:45:51.148662Z","shell.execute_reply.started":"2025-01-04T12:45:50.634022Z","shell.execute_reply":"2025-01-04T12:45:51.148022Z"}},"outputs":[{"name":"stdout","text":"Found 18345 images belonging to 10 classes.\nFound 4585 images belonging to 10 classes.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Creating The VGG16 Model","metadata":{}},{"cell_type":"code","source":"vgg = VGG16(weights=\"imagenet\",include_top = False, input_shape = (224,224,3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:45:54.633111Z","iopub.execute_input":"2025-01-04T12:45:54.633388Z","iopub.status.idle":"2025-01-04T12:45:54.864690Z","shell.execute_reply.started":"2025-01-04T12:45:54.633369Z","shell.execute_reply":"2025-01-04T12:45:54.864030Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"for layer in vgg.layers:\n    layer.trainable = False\n\nx = vgg.output\nx  = Flatten()(x)\nx = Dense(units = 1000, activation = \"relu\")(x)\nx = Dense(units = 10, activation = \"softmax\")(x)\nmodel = Model(inputs = vgg.input,outputs= x)\n\n# Compiling the Model\nmodel.compile(\n    optimizer = \"adam\",\n    loss = [\"categorical_crossentropy\"],\n    metrics = [\"accuracy\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:46:01.329753Z","iopub.execute_input":"2025-01-04T12:46:01.330094Z","iopub.status.idle":"2025-01-04T12:46:01.354826Z","shell.execute_reply.started":"2025-01-04T12:46:01.330065Z","shell.execute_reply":"2025-01-04T12:46:01.354152Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Creating Callback to save the Best Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\nmodel_checkpoint  = ModelCheckpoint(\n    \"best_model.keras\",\n    monitor = \"val_accuracy\",\n    mode = \"max\",\n    save_best_only = True,\n    verbose = 1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:46:04.813453Z","iopub.execute_input":"2025-01-04T12:46:04.813739Z","iopub.status.idle":"2025-01-04T12:46:04.817741Z","shell.execute_reply.started":"2025-01-04T12:46:04.813717Z","shell.execute_reply":"2025-01-04T12:46:04.816776Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Training the Model using train dataset","metadata":{}},{"cell_type":"code","source":"model.fit(\n    train_data,\n    validation_data = test_data,\n    epochs = 10,\n    callbacks = [model_checkpoint],\n    verbose = 1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:46:07.859308Z","iopub.execute_input":"2025-01-04T12:46:07.859580Z","iopub.status.idle":"2025-01-04T13:23:02.955842Z","shell.execute_reply.started":"2025-01-04T12:46:07.859560Z","shell.execute_reply":"2025-01-04T13:23:02.954965Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - accuracy: 0.5500 - loss: 2.4518\nEpoch 1: val_accuracy improved from -inf to 0.82028, saving model to best_model.keras\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 390ms/step - accuracy: 0.5502 - loss: 2.4496 - val_accuracy: 0.8203 - val_loss: 0.5372\nEpoch 2/10\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step - accuracy: 0.7725 - loss: 0.6429\nEpoch 2: val_accuracy did not improve from 0.82028\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 381ms/step - accuracy: 0.7725 - loss: 0.6428 - val_accuracy: 0.8070 - val_loss: 0.5461\nEpoch 3/10\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - accuracy: 0.8112 - loss: 0.5420\nEpoch 3: val_accuracy improved from 0.82028 to 0.85649, saving model to best_model.keras\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 389ms/step - accuracy: 0.8112 - loss: 0.5420 - val_accuracy: 0.8565 - val_loss: 0.4116\nEpoch 4/10\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 0.8314 - loss: 0.4859\nEpoch 4: val_accuracy did not improve from 0.85649\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 378ms/step - accuracy: 0.8314 - loss: 0.4859 - val_accuracy: 0.8436 - val_loss: 0.4597\nEpoch 5/10\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 0.8422 - loss: 0.4604\nEpoch 5: val_accuracy improved from 0.85649 to 0.87721, saving model to best_model.keras\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 381ms/step - accuracy: 0.8422 - loss: 0.4603 - val_accuracy: 0.8772 - val_loss: 0.3511\nEpoch 6/10\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - accuracy: 0.8400 - loss: 0.4629\nEpoch 6: val_accuracy did not improve from 0.87721\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 381ms/step - accuracy: 0.8400 - loss: 0.4629 - val_accuracy: 0.8236 - val_loss: 0.5082\nEpoch 7/10\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 0.8525 - loss: 0.4283\nEpoch 7: val_accuracy did not improve from 0.87721\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 378ms/step - accuracy: 0.8525 - loss: 0.4283 - val_accuracy: 0.8728 - val_loss: 0.3778\nEpoch 8/10\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step - accuracy: 0.8594 - loss: 0.4158\nEpoch 8: val_accuracy did not improve from 0.87721\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 380ms/step - accuracy: 0.8594 - loss: 0.4158 - val_accuracy: 0.8209 - val_loss: 0.5505\nEpoch 9/10\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - accuracy: 0.8602 - loss: 0.3917\nEpoch 9: val_accuracy did not improve from 0.87721\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 376ms/step - accuracy: 0.8602 - loss: 0.3917 - val_accuracy: 0.8632 - val_loss: 0.4447\nEpoch 10/10\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - accuracy: 0.8770 - loss: 0.3602\nEpoch 10: val_accuracy did not improve from 0.87721\n\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 375ms/step - accuracy: 0.8770 - loss: 0.3602 - val_accuracy: 0.8656 - val_loss: 0.3920\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x78dea8468400>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:45:43.076258Z","iopub.status.idle":"2025-01-04T12:45:43.076570Z","shell.execute_reply":"2025-01-04T12:45:43.076450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = test_data.class_indices.keys()  # or test_data.classes if you want to get class labels as integers\n\n# Convert to a list if you need it\nclass_names = list(class_names)\nprint(class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T13:31:17.411375Z","iopub.execute_input":"2025-01-04T13:31:17.411677Z","iopub.status.idle":"2025-01-04T13:31:17.416446Z","shell.execute_reply.started":"2025-01-04T13:31:17.411654Z","shell.execute_reply":"2025-01-04T13:31:17.415369Z"}},"outputs":[{"name":"stdout","text":"['Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"### Preprocessing the data For Prediction","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import img_to_array,load_img\n\ndef predict(model,img_path):\n    img = load_img(img_path,target_size = (224,224))\n    img_array  = img_to_array(img)\n    img_array = img_array/255      # Normalizing the image\n    img_array = np.expand_dims(img_array,axis=0)\n    prediction = model.predict(img_array)\n    prediction = np.argmax(prediction)\n    prediction = class_names[prediction]\n    print(prediction)\n    return prediction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:45:43.079187Z","iopub.status.idle":"2025-01-04T12:45:43.079482Z","shell.execute_reply":"2025-01-04T12:45:43.079365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_path = \"/kaggle/input/tomato/New Plant Diseases Dataset(Augmented)/valid/Tomato___Leaf_Mold/146ceb05-4beb-4a1c-9ba5-233f9f1ff0fa___Crnl_L.Mold 6985.JPG\"\npredict(model,img_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:45:43.080588Z","iopub.status.idle":"2025-01-04T12:45:43.080871Z","shell.execute_reply":"2025-01-04T12:45:43.080758Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving the Final Model Using Tensor Lite","metadata":{}},{"cell_type":"code","source":"model.save(\"model.h5\",include_optimizer = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T12:45:43.081740Z","iopub.status.idle":"2025-01-04T12:45:43.082025Z","shell.execute_reply":"2025-01-04T12:45:43.081890Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Saving model Using tensorflow lite\nfrom tensorflow.keras.models import load_model\nmodel = load_model(\"/kaggle/working/best_model.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T13:26:14.269734Z","iopub.execute_input":"2025-01-04T13:26:14.270089Z","iopub.status.idle":"2025-01-04T13:26:15.741485Z","shell.execute_reply.started":"2025-01-04T13:26:14.270059Z","shell.execute_reply":"2025-01-04T13:26:15.740580Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import tensorflow as tf\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T13:28:34.641225Z","iopub.execute_input":"2025-01-04T13:28:34.641551Z","iopub.status.idle":"2025-01-04T13:28:34.645292Z","shell.execute_reply.started":"2025-01-04T13:28:34.641525Z","shell.execute_reply":"2025-01-04T13:28:34.644445Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"converter.optimizations = [tf.lite.Optimize.DEFAULT]\nlite_model = converter.convert()\nwith open(\"lite_model.tflite\",\"wb\") as f:\n    f.write(lite_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-04T13:30:11.137533Z","iopub.execute_input":"2025-01-04T13:30:11.137933Z","iopub.status.idle":"2025-01-04T13:30:20.768055Z","shell.execute_reply.started":"2025-01-04T13:30:11.137902Z","shell.execute_reply":"2025-01-04T13:30:20.767292Z"}},"outputs":[{"name":"stdout","text":"Saved artifact at '/tmp/tmp7xwt41jt'. The following endpoints are available:\n\n* Endpoint 'serve'\n  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer_2')\nOutput Type:\n  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\nCaptures:\n  132897698391216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897698441072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897698445296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897698439664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897698434384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897698440896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897698435616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897698431392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897698434736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897700714576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897700722144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897700712640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897697795936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897697802096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897697796464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897697806320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897697800512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897697794176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897697796992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897697805440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897569868256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897569870544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897569865968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897569873008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897569868432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897569875472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897569876352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897569880048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897569876880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132897569878992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}